<!DOCTYPE html>

<html lang="en">
    <head>

        <!-- Metadata -->
        <meta charset="utf-8"/>
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/>
        <meta name="description" content="website"/>
        <meta name="author" content="yuguang li"/>
        <meta name="google-site-verification" content="7HNEDILeOTNSbayuMAqacCGFRCQ3zMTki590sHdlFbs" />
        <title>Yuguang Li | CV/Graphics researcher @ UW GRAIL & Zillow Group</title>
        <link rel="icon" type="image/x-icon" href="assets/img/mandu_icon.png"/>
        
        <!-- Font Awesome icons -->
        <script src="https://use.fontawesome.com/releases/v5.15.3/js/all.js"></script>
		
        <!-- Google fonts-->
        <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css">        
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
        <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet" type="text/css"/>
        <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet" type="text/css"/>
        <link href="https://fonts.googleapis.com/css2?family=Fira+Code&display=swap" rel="stylesheet">
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Boldonse&family=ZCOOL+QingKe+HuangYou&display=swap" rel="stylesheet">
        <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/OwlCarousel2/2.3.4/assets/owl.carousel.min.css">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/OwlCarousel2/2.3.4/assets/owl.theme.default.min.css">

        <!-- Core theme CSS -->
        <link href="styles/styles.css" rel="stylesheet"/>
        
        <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>
        <script src="js/scripts.js"></script>

    </head>

    <body>
        <!-- Moving particles -->
        <canvas id="canvas"></canvas>

        <!-- Progress bar on top -->
        <div class="progress-bar-container">
            <div class="progress-bar" id="progressBar"></div>
        </div>

        <!-- Back to top button -->
        <a id="back-to-top-button"></a>

         <!-- Content -->
         <div class="container" style="padding-top: 3rem;">
            <div class="row mb-4">
                <div class="col-lg-2 col-md-4">
                    <div class="ring-container">
                        <div class="ring">
                            <div class="hollow-ring">
                                <img class="profile-image" src="assets/profile.jpg" alt="yuguang li"/>
                                
                            </div>
                        </div>
                    </div>
                    <hr />
                    <div class="social-icons">
                         <a class="social-icon" href="https://scholar.google.com/citations?user=B8oOu2wAAAAJ&hl=en" target="_blank" title="Google Scholar"><i class="fa fa-graduation-cap" style="font-size: 25px; color: #4285f4"></i></a>
                         <!-- <a class="social-icon" href="xx" target="_blank" rel="noopener" title="Semantic Scholar"><i class="fa fa-graduation-cap" style="font-size: 35px; color: #4285f4"></i></a> -->
                         <span class="divider">|</span>
                         <a class="social-icon" href="https://github.com/yuguang-li" target="_blank" title="GitHub"><i class="fab fa-github" style="font-size: 25px; color: #171515"></i></a> 
                         <span class="divider">|</span>
                         <!-- <a class="social-icon" href="https://patents.justia.com/inventor/yuguang-li" target="_blank" title="Patents"><i class="fa-solid fa-file-certificate" style="font-size: 25px; color: #4285f4"></i></a>  -->
                         <a class="social-icon" href="https://patents.justia.com/inventor/yuguang-li" target="_blank" title="Patents"><img width="25" height="auto" class="w-full rounded-lg" src="assets/patented.jpg" /></a> 
                         <span class="divider">|</span>
                         <a class="social-icon" href="mailto:ylee3@cs.washington.edu" title="Email">
                            <i class="fa fa-envelope" style="font-size: 25px; color: #4285f4"></i>
                        </a>
                    </div>
                    <p></p>
                </div>
            
                <!-- Re-written, redesigned, and integrated (from HTML (Pug) and CSS (Less) to pure HTML and CSS) -->
                <div class="contact-card-overlay" id="overlay-bg">
                    <div class="information_card">
                        <div id="front_end_card">
                            <div class="back">
                                <div class="contact-info-card">
                                    <p class="card-name">Yuguang Li</p>
                                    <a href="https://yuguang-li.com/" target="_blank" rel="noopener" class="card-website-link" title="Website">yuguang-li.com</a>
                                </div>
                            </div>
            
                        </div>
                    </div>
                </div> 
            
                <div class="col-lg-10 col-md-8">
                    <h2 class="name">
                        Yuguang Li <span id="chinese">(李宇光)</span>
                    </h2>
                    <p></p>   
                    I am a full-time researcher at the <a href="https://grail.cs.washington.edu/" target="_blank" rel="noopener">GRAIL Lab</a>, University of Washington, working on the final chapter of my PhD thesis. Concurrently, I work as a Principal Research Scientist at <a href="https://www.zillow.com/z/3d-home/" target="_blank" rel="noopener">Zillow Group</a>.
                    I earned my Bachelor's degree in Opto-Electronics Engineering, during which I developed ray-tracing models for large vegetated scenes using CUDA and published three papers in top-tier remote sensing journals. 
                    
                    <p></p>
                    In recent years, I've been exploring research in the areas of computer vision, machine learning and computer graphcis, within industrial environments. Our work has primarily focused on enhancing automation and efficiency in indoor reconstruction, with a strong emphasis on robustness and practicality. 
                    This research has resulted in dozens of first-author patents and several top-tier papers. I had the privilege of leading <a href="https://www.zillow.com/z/3d-home/floor-plans/" target="_blank" rel="noopener">Zillow's indoor reconstruction project</a>, starting with a small team of scientists, and expanding the production pipeline to reconstruct hundreds of thousands of homes annually with a high degree of automation and an offshore QA team. I was featured in Zillow's  <a href="https://www.zillowgroup.com/news/how-scientists-at-zillow-are-making-your-home-shopping-journey-faster-and-more-accurate/" target="_blank" rel="noopener">article</a>  as a result of this work. We solve precise camera poses and precise indoor structure from unposed RGB panorama images with extreme low capture densities. 
                    Our iterations started off by exploring a graphics focus human-in-the-loop pipeline (ZInD), to single / few-image layout estimation (PSMNet), two-view coarse camera pose estimation (Salve / CovisPose) and full scale learned bundle adjustments from dozens of input panorama images (BADGR). 
                    <p></p>
                    
                    I've decided to focus on academia and complete my PhD dessertation from University of Washington in 2024, while still contributing to Zillow. 
                    I'm fortunate to take advise from professor <a href="https://homes.cs.washington.edu/~shapiro/" target="_blank" rel="noopener">Linda Shapiro</a>, 
                    <a href="https://www.colburn.org/" target="_blank" rel="noopener">Alex Colburn</a>, 
                    <a href="https://www.singbingkang.com/" target="_blank" rel="noopener">Sing Bing Kang</a> and 
                    <a href="https://ranjaykrishna.com/index.html" target="_blank" rel="noopener">Ranjay Krishna</a>. 
                    My recent research focuses on solving precise multi-view geometry and camera poses. Particularly, we explored to constrain non-linear optimization process with the learned priors from generative models in an end-to-end trainable fashion to avoid conflicting gradients. I've also been exploring reconstructing high-fidelity 3D scenes for novel view synthesis from few-shot image inputs, where visual details like highlights and shadows from light sources and materials are preserved.
                    
                    <p></p>
                </div>

                <div class="row" id="research">
                    <div class="col">
                        <h2 class="mb-5">Academic Papers</h2>
                        <p></p>
                        <!-- <div id="filters-project">
                            <button class="filter-button active" data-filter="*">all</button>
                            
                                <button class="filter-button" data-filter="perception manipulation">perception + manipulation</button>
                            
                                <button class="filter-button" data-filter="slam">slam</button>
                            
                                <button class="filter-button" data-filter="framework">framework</button>
                            
                        </div> -->
                        <p></p>
                        <div id="projects" class="isotope">
                                <div class="project" data-filter="slam">
                    <div class="row mb-4">
                        <div class="col-sm-4">
                            <img width="300" height="auto" class="w-full rounded-lg" src="assets/badgr.png" />
                        </div>
                        <div class="col-sm-8 project">
                            
                            <b>BADGR: Bundle Adjustment Diffusion Conditioned by GRadients for Wide-Baseline Floor Plan Reconstruction</b>
                            <br />
                            <i><a href="xx" target="_blank" rel="noopener">Conference on Computer Vision and Pattern Recognition (CVPR) 2025</a></i> 
                            <i><b>Highlight (top 13%)</b></i> 
                            
                            <br />
                            Yuguang Li, Ivaylo Boyadzhiev, Zixuan Liu, Linda Shapiro, Alex Colburn.
                            <br />
                            <a href="https://badgr-diffusion.github.io/" target="_blank" rel="noopener">[Project]</a>  
                            <a href="https://arxiv.org/abs/2503.19340" target="_blank" rel="noopener">[Paper]</a> 
                             | <a href="git" target="_blank" rel="noopener">[Code]</a>  
                             <!-- | <a href="xx" target="_blank" rel="noopener">[POSTER]</a>  -->
                            <br />
                            <u><b><i>Abstract</i></b>:</u> 
                            Reconstructing precise camera poses and floor plan layouts from a set of wide-baseline RGB panoramas is a difficult and unsolved problem. We present BADGR, 
                            a novel diffusion model which performs both reconstruction and bundle adjustment (BA) optimization tasks, 
                            to refine camera poses and layouts from a given coarse state using 1D floor boundary information from dozens of images of
                            varying input densities. 
                            <span class="collapse" id="badgr">
                                Unlike a guided diffusion model, BADGR is conditioned on dense per-feature outputs
                            from a single-step Levenberg Marquardt (LM) optimizer and is trained to predict camera and wall positions while
                            minimizing reprojection errors for view-consistency. The objective of layout generation from denoising diffusion process complements BA optimization by providing additional
                            learned layout-structural constraints on top of the co-visible features across images. These constraints help BADGR to
                            make plausible guesses on spatial relations which help constrain pose graph, such as wall adjacency, collinearity, and
                            learn to mitigate errors from dense boundary observations with global contexts. BADGR trains exclusively on 2D floor
                            plans, simplifying data acquisition, enabling robust augmentation, and supporting variety of input densities. Our
                            experiments and analysis validate our method, which significantly outperforms the state-of-the-art pose and floor plan
                            layouts reconstruction with different input densities.
                            </span> 
                            <span> <a href="#badgr" data-toggle="collapse" onclick="toggleText(this)" id="link-badgr">... See More</a></span>
                        </div>                       
                    </div>

                    <hr />

                    <div class="row mb-4">
                        <div class="col-sm-4">
                            <img width="300" height="auto" class="w-full rounded-lg" src="assets/covispose.png" />
                        </div>
                        <div class="col-sm-8 project">
                            
                            <b>CoVisPose: Co-Visibility Pose Transformer for Wide-Baseline Relative Pose Estimation in 360 Indoor Panoramas</b>
                            <br />
                            <i><a href="xx" target="_blank" rel="noopener">European Conference on Computer Vision ECCV 2022</a></i> 
                            
                            <br />
                            Will Hutchcroft, Yuguang Li, Ivaylo Boyadzhiev, Zhiqiang Wan, Haiyan Wang, Sing Bing Kang.
                            <br />
                            <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136920610.pdf" target="_blank" rel="noopener">[Main Paper]</a> 
                            <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136920610-supp.pdf" target="_blank" rel="noopener">[Supplementary]</a> 
                             | <a href="git" target="_blank" rel="noopener">[Code]</a> 
                            <br />
                            <u><b><i>Abstract</i></b>:</u> 
                            We present CoVisPose,an end-to-end supervised learning method for relative camera pose estimation in wide baseline 360 indoor
                            panoramas. To address the challenges of occlusion, perspective changes, and textureless or repetitive regions, we generate rich representations
                            for direct pose regression by jointly learning dense bidirectional visual overlap, correspondence, and layout geometry. 
                            <span class="collapse" id="covispose">
                                We estimate three image column-wise quantities: co-visibility (the probability that a given column’s
                            image content is seen in the other panorama), angular correspondence
                            (angular matching of columns across panoramas), and floor layout (the
                            vertical floor-wall boundary angle). We learn these dense outputs by
                            applying a transformer over the image-column feature sequences, which
                            cover the full 360 field-of-view (FoV) from both panoramas.The result an
                            rich representation supports learning robust relative poses with an efficient
                            1D convolutional decoder. In addition to learned direct pose regression
                            with scale,our network also supports pose estimation through a RANSAC-
                            based rigid registration of the predicted corresponding layout boundary
                            points. Our method is robust to extremely wide baselines with very low
                            visual overlap, as well as significant occlusions. We improve upon the
                            SOTA by a large margin, as demonstrated on a large-scale dataset of
                            real homes, ZInD.
                            </span> 
                            <span> <a href="#covispose" data-toggle="collapse" onclick="toggleText(this)" id="link-covispose">... See More</a></span>
                        </div>                       
                    </div>

                    <hr />

                    <div class="row mb-4">
                        <div class="col-sm-4">
                            <img width="300" height="auto" class="w-full rounded-lg" src="assets/salve.png" />
                        </div> 
                        <div class="col-sm-8 project">
                            
                            <b>SALVe: Semantic Alignment Verification for Floorplan Reconstruction from Sparse Panoramas</b>
                            <br />
                            <i><a href="xx" target="_blank" rel="noopener">European Conference on Computer Vision ECCV 2022</a></i> 
                            <br />
                            John Lambert, Yuguang Li, Ivaylo Boyadzhiev, Lambert Wixson, Manjunath Narayana, Will Hutchcroft, James Hays, Frank Dellaert, Sing Bing Kang.
                            <br />
                            <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136910632.pdf" target="_blank" rel="noopener">[Main Paper]</a> 
                            <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136910632-supp.pdf" target="_blank" rel="noopener">[Supplementary]</a> 
                            |<a href="https://github.com/zillow/salve" target="_blank" rel="noopener">[Code]</a> 
                            <br />
                            <u><b><i>Abstract</i></b>:</u> 
                            We propose a new system for automatic 2D floorplan reconstruction that is enabled by SALVe, our novel pairwise learned alignment verifier. 
                            The inputs to our system are sparsely located 360 panoramas, whose semantic features
                            (windows, doors, and openings) are inferred and used to hypothesize pairwise
                            room adjacency or overlap. 
                            <span class="collapse" id="salve">
                                SALVe initializes a pose graph, which is subsequently
                            optimized using GTSAM. Once the room poses are computed, room layouts
                            are inferred using HorizonNet, and the floorplan is constructed by stitching
                            the most confident layout boundaries. We validate our system qualitatively and
                            quantitatively as well as through ablation studies, showing that it outperforms
                            state-of-the-art SfM systems in completeness by over 200%, without sacrific-
                            ing accuracy. Our results point to the significance of our work: poses of 81% of
                            panoramas are localized in the first 2 connected components (CCs), and 89% in
                            the first 3 CCs.
                            </span> 
                            <span> <a href="#salve" data-toggle="collapse" onclick="toggleText(this)" id="link-salve">... See More</a></span>
                        </div>                       
                    </div>

                    <hr />

                    <div class="row mb-4">
                        <div class="col-sm-4">
                            <img width="250" height="auto" class="w-full rounded-lg" src="assets/psmnet.png" />
                        </div>
                        <div class="col-sm-8 project">
                            
                            <b>PSMNet: Position-aware Stereo Merging Network for Room Layout Estimation</b>
                            <br />
                            <i><a href="xx" target="_blank" rel="noopener">Conference on Computer Vision and Pattern Recognition (CVPR) 2022</a></i> 
                            <br />
                            Haiyan Wang, Yuguang Li, Will Hutchcroft, Zhiqiang Wan, Ivaylo Boyadzhiev, Yingli Tian, Sing Bing Kang.
                            <br />
                             <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Wang_PSMNet_Position-Aware_Stereo_Merging_Network_for_Room_Layout_Estimation_CVPR_2022_paper.pdf" target="_blank" rel="noopener">[Paper]</a> 
                             <a href="https://github.com/zillow/psmnet-layout" target="_blank" rel="noopener">[Code]</a> 
                            <br />
                            <u><b><i>Abstract</i></b>:</u> 
                            In this paper, we propose a new deep learning-based method for estimating room layout given a pair of 360
                            panoramas. Our system, called Position-aware Stereo Merging Network or PSMNet, is an end-to-end joint layout-pose
                            estimator. 
                            <span class="collapse" id="psmnet">
                                PSMNet consists of a Stereo Pano Pose (SP2) transformer and a novel Cross-Perspective Projection (CP2)
                            layer. The stereo-view SP2 transformer is used to implicitly infer correspondences between views, and can handle noisy
                            poses. The pose-aware CP2 layer is designed to render features from the adjacent view to the anchor (reference)
                            view, in order to perform view fusion and estimate the visible layout. Our experiments and analysis validate our method,
                            which significantly outperforms the state-of-the-art layout estimators, especially for large and complex room spaces.
                            </span> 
                            <span> <a href="#psmnet" data-toggle="collapse" onclick="toggleText(this)" id="link-psmnet">... See More</a></span>
                        </div>                       
                    </div>

                    <hr />

                    <div class="row mb-4">
                        <div class="col-sm-4">
                            <img width="300" height="auto" class="w-full rounded-lg" src="assets/zind.png" />
                        </div>
                        <div class="col-sm-8 project">
                            
                            <b>Zillow Indoor Dataset: Annotated Floor Plans With 360 Panoramas and 3D Room Layouts</b>
                            <br />
                            <i><a href="xx" target="_blank" rel="noopener">Conference on Computer Vision and Pattern Recognition (CVPR) 2021</a></i> 
                            <br />
                            Steve Cruz, Will Hutchcroft, Yuguang Li, Naji Khosravan, Ivaylo Boyadzhiev, Sing Bing Kang.
                            <br />
                             <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Cruz_Zillow_Indoor_Dataset_Annotated_Floor_Plans_With_360deg_Panoramas_and_CVPR_2021_paper.pdf" target="_blank" rel="noopener">[Main Paper]</a> 
                             <a href="https://openaccess.thecvf.com/content/CVPR2021/supplemental/Cruz_Zillow_Indoor_Dataset_CVPR_2021_supplemental.pdf" target="_blank" rel="noopener">[Supplementary]</a> 
                             <a href="https://github.com/zillow/zind" target="_blank" rel="noopener">[Code]</a> 
                             <a href="https://www.zillow.com/tech/zillow-indoor-dataset-facilitates-better-3d-tours/" target="_blank" rel="noopener">[Article]</a> 
                            <br />
                            <u><b><i>Abstract</i></b>:</u> 
                            We present Zillow Indoor Dataset (ZInD): A large indoor dataset with 71,474 panoramas from 1,524 real unfurnished homes.
                            ZInD provides annotations of 3D room layouts, 2D and 3D floor plans, panorama location in the floor
                            plan, and locations of windows and doors. 
                            <span class="collapse" id="zind">
                                The ground truth construction took over 1,500 hours of annotation work. To
                            the best of our knowledge, ZInD is the largest real dataset with layout annotations. 
                            A unique property is the room layout data, which follows a real world distribution (cuboid,
                            more general Manhattan, and non-Manhattan layouts) as opposed to the mostly cuboid or Manhattan layouts in current publicly available datasets. 
                            Also, the scale and annotations provided are valuable for effective research related to
                            room layout and floor plan analysis. To demonstrate ZInD’s benefits, we benchmark on room layout 
                            estimation from single panoramas and multi-view registration.
                            </span> 
                            <span> <a href="#zind" data-toggle="collapse" onclick="toggleText(this)" id="link-zind">... See More</a></span>
                        </div>                       
                    </div>

                    <hr />

                    <div class="row mb-4">
                        <div class="col-sm-4">
                            <img width="300" height="auto" class="w-full rounded-lg" src="assets/raytracing.png" />
                        </div>
                        <div class="col-sm-8 project">
                            
                            <b>Simulated impact of sensor field of view and distance on field measurements of bidirectional reflectance factors for row crops.</b>
                            <br />
                            <i><a href="https://www.sciencedirect.com/journal/remote-sensing-of-environment" target="_blank" rel="noopener">Remote Sensing of Environment (2015)</a></i> 
                            <br />
                            Feng Zhao, Yuguang Li, Xu Dai, Wout Verhoef, Yiqing Guo, Hong Shang, Xingfa Gu, Yanbo Huang, Tao Yu, Jianxi Huang. 
                            <br />
                             <a href="https://www.sciencedirect.com/science/article/pii/S003442571400354X" target="_blank" rel="noopener">[Paper]</a> 
                            <br />
                            <u><b><i>Abstract</i></b>:</u> 
                                We built a Monte Carlo ray-tracing engine with weighted sampling to simulate radiation transfer in architecturally realistic canopies.
                            <span class="collapse" id="laigd">
                                <br /> It is well established that a natural surface exhibits anisotropic reflectance properties that depend on the characteristics of the surface. 
                            Spectral measurements of the bidirectional reflectance factor (BRF) at ground level provide us a method to capture the directional characteristics of the observed surface. 
                            Various spectro-radiometers with different field of views (FOVs) were used under different mounting conditions to measure crop reflectance. 
                            The impact and uncertainty of sensor FOV and distance from the target have rarely been considered. 
                            The issue can be compounded with the characteristic reflectance of heterogeneous row crops. Because of the difficulty of accurately 
                            obtaining field measurements of crop reflectance under natural environments, a method of computer simulation was proposed to study the impact 
                            of sensor FOV and distance on field measured BRFs. A Monte Carlo model was built to combine the photon spread method and the weight reduction concept to develop the 
                            weighted photon spread (WPS) model to simulate radiation transfer in architecturally realistic canopies. Comparisons of the Monte Carlo model with both 
                            field BRF measurements and the RAMI Online Model Checker (ROMC) showed good agreement. BRFs were then simulated for a range of sensor FOV and distance combinations 
                            and compared with the reference values (distance at infinity) for two typical row canopy scenes. Sensors with a finite FOV and distance from the target approximate 
                            the reflectance anisotropy and yield average values over FOV. Moreover, the perspective projection of the sensor causes a proportional distortion in the sensor FOV from the ideal 
                            directional observations. Though such factors inducing the measurement error exist, it was found that the BRF can be obtained with a tolerable bias on ground level with a proper combination 
                            of sensor FOV and distance, except for the hotspot direction and the directions around it. Recommendations for the choice of sensor FOV and distance are also made to reduce the bias from the real 
                            angular signatures in field BRF measurement for row crops.
                            </span> 
                            <span> <a href="#laigd" data-toggle="collapse" onclick="toggleText(this)" id="link-laigd">... See More</a></span>
                        </div> 
                    </div>

                    <hr />

                    <div class="row mb-4">
                        <div class="col-sm-4">
                            <img width="300" height="auto" class="w-full rounded-lg" src="assets/LAIGD.png" />
                        </div>
                        <div class="col-sm-8 project">
                            
                            <b>A linearly approximated iterative Gaussian decomposition method for waveform LiDAR processing</b>
                            <br />
                            <i><a href="xx" target="_blank" rel="noopener">ISPRS Journal of Photogrammetry and Remote Sensing (2017)</a></i> 
                            <br />
                            Yuguang Li, Giorgos Mountrakis.
                            <br />
                             <a href="https://www.sciencedirect.com/science/article/abs/pii/S092427161730059X" target="_blank" rel="noopener">[Paper]</a> 
                            <br />
                            <u><b><i>Abstract</i></b>:</u> 
                            Full-waveform LiDAR (FWL) decomposition results often act as the basis for key LiDAR-derived products. We propose LAIGD, which follows a multi-step
                            "slow-and-steady” iterative structure, where new Gaussian nodes are quickly discovered and adjusted
                            using a linear fitting technique before they are forwarded for a non-linear optimization. 
                            <span class="collapse" id="laigd"> 
                                Two experiments
                            were conducted along with another using synthetic data. LVIS data revealed considerable improvements in RMSE
                            (44.8% lower), RSE (56.3% lower) and rRMSE (74.3% lower) values compared to the benchmark. These results were further confirmed with the synthetic data. Furthermore, the proposed
                            multi-step method reduces execution times in half.
                            </span> 
                            <span> <a href="#laigd" data-toggle="collapse" onclick="toggleText(this)" id="link-laigd">... See More</a></span>
                        </div>                       
                    </div> 
                    
                    <h2 class="mb-5">Other Papers</h2>

                    <div class="row mb-4 other">
                        <b>FluorWPS: A Monte Carlo ray-tracing model to compute sun-induced chlorophyll fluorescence of three-dimensional canopy.</b> Remote Sensing of Environment 187 (2016): 385-399.   
                    </div>
                    
                    <div class="row mb-4 other">
                        <b>Graph-Covis: GNN-based multi-view panorama global pose estimation.</b> Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2023.
                    </div>

                    <div class="row mb-4 other">
                        <b>U2rle: Uncertainty-guided 2-stage room layout estimation.</b> Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.
                    </div>
                    
                    <div class="row mb-4 other">
                        <b>Efficient and accurate mitosis detection - A lightweight RCNN approach.</b> International Conference on Pattern Recognition Applications and Methods. 2018.
                    </div>
                    
                    <div class="row mb-4 other">
                        <b>GPU-based acceleration for Monte Carlo Ray-Tracing of complex scene.</b> IEEE International Geoscience and Remote Sensing Symposium. 2012.
                    </div>

                    <div class="row mb-4 other">
                        <b>A computer simulation model to compute the radiation transfer of mountainous regions.</b> SPIE Remote Sensing, 2011.
                    </div>
                    
                    
                </div>
            </div>
        </div>

    </body>
</html>